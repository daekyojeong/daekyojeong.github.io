---
title: "Feature engineering : 비정형 데이터"
author: Daekyo Jeong
date: 2021-04-14 15:00:00 +0900
categories: [AI, Theory]
tags: [IT, AI, Machine Learning, Deep Learning]

math: true
---

---
**Contents**

{:.no_toc}

* Will be replaced with the ToC, excluding the "Contents" header
{:toc}
---

<br/>

# **비정형 데이터**  

정형 데이터의 반대되는 경우로, 하나의 수치로 표현하기 힘들고 값의 의미를 쉽게 파악하기 힘든 데이터들을 말한다.  

텍스트, 이미지, 음성, 영상 등의 데이터들이 여기에 해당한다.  


# **텍스트 데이터**  

### **BOW(Bag of words), N-gram**  

가장 기본적인 텍스트 표현 방법이다.  

텍스트를 순서에 상관없이 단어 단위로 분해하여 벡터에 저장한다.  

벡터의 각 차원은 하나의 단어를 나타내고, 각 차원에 대응하는 가중치는 해당 단어가 문장에서 얼마나 중요한지를 나타낸다.  

일반적으로 TF-IDF를 사용하여 가중치를 계산하며, 수식은 다음과 같다.  

$$
TF_IDF(t,d) = TF(t,d) \times IDF(t)  
$$

TF(t, d)는 특정 단어 t가 문장 d에서 몇 번 나타났는지 빈도를 의미한다.  

IDF(t)는 역빈도로써, 단어 t가 의미를 전달함에 있어 어떤 중요도를 가지는가를 연산한다.  

IDF를 구하는 공식은 다음과 같다.  

$$
IDF(t) = log\frac{총 문장 수}{단어 t를 포함하는 문장 수 + 1}
$$

즉, 단어 t가 많은 문자에 출현한다면 해당 단어는 비교적 자주 사용하는 단어가 된다.  

따라서, 어떠한 문장에서 특수한 의미를 구별하는데 크게 기여하지 못할 가능성이 높기 때문에, 가중치에 패널티를 부여하는 것이다.  

여기서 문제는 문장을 단어 단위로 구별하는 것이 과연 바람직한 방법일까?  

여러 단어들이 합쳐졌을 때 다른 의미를 가지는 경우들도 많다.  

이러한 문제를 해결하기 위해 N-gram 이라는 방법을 많이 이용한다.  

N-gram은 n개의 단어를 하나의 그룹으로 구성하여 피처를 생성한다.  

이 외에도 동일한 의미를 가진 단어지만, 다양한 품사로 형태가 변형될 수 있다.  

이를 방지하기 위해 어간 추출(word stemming)을 진행하여 단어를 하나의 어간으로 통일시켜 준다.  


### **워드 임베딩 및 딥러닝 모델**  

워드 임베딩이란 단어를 정량화하는 작업이다.  

핵심 아이디어는 단어를 저차원 공간상의 고밀도 벡터(dense vector)로 투영하는 것이다.  

워드 임베딩은 각각의 단어를 K 차원의 벡터에 투영하기 때문에, N개의 단어가 존재할 때 N x K 차원의 행렬을 사용하여 표현할 수 있다.  

하지만 이러한 표현은 단어가 가지는 고차원적인 의미를 잡아내긴 힘들다는 단점이 존재한다.  

워드 임베딩한 결과만을 머신러닝의 입력으로 사용한다면 만족할만한 결과를 얻기가 힘들 것이다.  

하지만 딥러닝 모델은 일종의 자동적인 피처 엔지니어링 프로세스를 가지고 있다.  

각각의 은닉층들은 서로 다른 단계의 추상 피처를 반영하기 때문에 더욱 쉽게 단어들의 고차원적인 의미를 잡아낼 수 있다.  

##### **Word2Vec**  

Word2Vec은 현재까지 가장 많이 사용되는 워드 임베딩 모델 중 하나이다.  

Word2Vec은 은닉층이 한개인 신경망 모델의 한 종류이며, CBOW와 Skip-gram으로 네트워크가 이루어져 있다.  

CBOW의 경우 주변에 출현하는 단어들을 기반으로 현재 단어의 생성확률을 예측하는 일을 한다.  

반대로 Skip-gram의 경우 현재 단어를 기반으로 주변 단어의 생성확률을 예측한다.  

w(t)를 현재 단어라고 했을 때, w(t-2), w(t-1), w(t+1), w(t+2)는 전후에 나오는 단어들이다.  

CBOW는 w(t-2), w(t-1), w(t+1), w(t+2)를 입력으로 받아 w(t)를 예측하며,  

Skip-gram은 w(t)를 입력으로 받아 w(t-2), w(t-1), w(t+1), w(t+2)를 예측하는 작업을 수행한다.  

그리고 앞뒤로 움직이는 슬라이딩 윈도우의 크기는 2이다.  

두 모델모두 입력층, 은닉층, 출력층으로 구성된다.  

출력층에서의 각 단어는 원핫 인코딩으로 표현된다.  

즉, 모든 단어가 N차원의 벡터로 이루어져 있으며, 각 단어와 대응되는 차원만 1이고 나머지는 0이다.  

모델의 자세한 내용과 다른 임베딩 모델들은 NLP 장에서 다루겠다.  

# **이미지 데이터**  

머신러닝에서 대부분의 모델은 대량의 데이터를 통해 학습을 진행한다.  

하지만 현실에서는 라벨링이 되어있는 데이터를 대량으로 구하기 힘든 경우가 많다.  

이런 경우에 어떻게 학습을 진행해야 할까?  

이미지 분류 문제에서 학습 데이터가 부족하다면 과적합(Overfitting) 문제가 발생할 것이다.  

이를 해결하기 위한 방안은 크게 두 가지가 있다.  

첫 번째는 모델의 간략화이다.  

이 방법에는 (정규화 추가, 앙상블 학습 사용, Drop out, 하이퍼 파라미터 조정) 등이 대표적이다.  

두 번째는 데이터에 기반을 둔 방법으로, 대표적으로 데이터 확장 방법이 있다.  

초기 데이터에 약간의 변형을 주어 데이터의 양을 늘리는 것이다.  

변형 방법은 다음과 같다.  

1. 일정 범위 내에서 이미지에 회전, 평행 이동, 축소, 확대, 삭제, 추가, 좌우 전환 등의 효과를 가한다.  

2. 이미지에 노이즈를 추가한다. (가우스 노이즈 등)  

3. 색상을 변환한다.  

4. 이미지의 명암, 해상도, 광도 등을 변환한다.  

이처럼 이미지를 직접적으로 변형하는 방식 외에도 먼저 이미지의 피처를 추출하고, 피처 공간 내에서 변환을 진행하여  

SMOTE(Synthetic Minority Oversampling Technique)와 같은 업샘플링을 이용할 수도 있다.  

최근에는 GANs과 같은 생성모델을 활용하여 새로운 데이터들을 만들기도 한다.  

그 외에도 이미 잘 훈련된 다른 모델을 가져와서 파인 튜닝만을 진행하는 전이 학습도 하나의 방법이다.  



<br/>
