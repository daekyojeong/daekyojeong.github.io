---
title: "모델 평가 3"
author: Daekyo Jeong
date: 2021-05-11 15:00:00 +0900
categories: [AI, Theory]
tags: [IT, AI, Machine Learning, Deep Learning]

math: true
---

---
**Contents**

{:.no_toc}

* Will be replaced with the ToC, excluding the "Contents" header
{:toc}
---

<br/>

# **모델 평가**  

### **하이퍼파라미터 튜닝**  

하이퍼파라미터는  모델 성능에 크게 영향을 주기 때문에 잘 다룰 수 있어야 한다.  
하지만 하이퍼파라미터의 최적값을 찾는 방법은 경험에 기반해 합리적인 값을 설정하는게 일반적이다.  

일반적으로 하이퍼파라미터를 최적화하기 위해 그리드 탐색, 랜덤 탐색, 베이지안 최적화 등의 알고리즘을 이용한다.  
각 알고리즘에 대해 알아보기 전에, 보통 탐색 알고리즘들이 포함하고 있는 요소들에 대해 알아보자.  
첫 번째는 목적함수이다.  
목적함수의 함수값을 최대화(이익, 점수)하거나, 최소화(비용, 에러)하여 최적해를 도출하는 문제를 의미한다.  
두 번째는 생성 가능한 신경망의 조합 수를 결정하기 위해 탐색 영역을 설계하는 탐색 범위 설정이다.  
마지막으로 세 번째는 성능을 측정하기 위해 하이퍼파라미터를 일정 간격으로 선택하는 탐색 스탭과 같은 기타 값이다.  

#### 그리드 탐색(Grid Search)  

그리드 탐색은 가장 간단하고 광범위하게 사용되는 하이퍼파라미터 탐색 알고리즘이다.  
탐색 범위의 모든 샘플을 대상으로 최적값을 찾는다.  
만약 탐색 범위를 비교적 크게 설정하고 탐색 스탭을 작게 설정했다면, 그리드 탐색으로 전역 최적해를 찾을 확률이 매우 높아진다.  
하지만 이 방식은 시간과 자원을 많이 사용해야 하며, 튜닝해야 하는 하이퍼파라미터가 많은 경우 비효율적이다.  
따라서 실제로는 탐색 범위와 스탭을 모두 크게 설정하여 전역 최적해가 존재할 것 같은 영역을 찾고, 범위와 스탭을 줄여나가며 더 정확한 최적해를 찾아나간다.  
이러한 방법은 시간과 자원을 절약할 수 있지만, 일반적으로 목적 함수가 컨벡스하지 않기 때문에 전역 최적해를 찾지 못할 가능성도 높다.  

#### 랜덤 탐색(Random Search)  

랜덤 탐색 아이디어는 그리드 탐색과 비슷하다.  
하지만 상한과 하한 사이의 모든 값에 대해 테스트하지 않고, 정해진 범위에서 랜덤으로 샘플 포인트를 선택한다.  
이 방법의 근거는 샘플의 수가 아주 많다면 랜덤 샘플링을 통해 전역 최적해 혹은 근사값을 찾을 가능성이 높다는 것이다.  
랜덤 탐색은 일반적으로 그리드 탐색보다 속도가 빠르지만, 결과가 최적해임을 보장할 수 없다는 단점이 있다.  

#### 베이지안 최적화(Bayesian Optimization)  

베이지안 최적화 알고리즘은 위의 두 탐색법과는 다른 새로운 방식을 이용한다.  
위의 두 탐색법은 하나의 새로운 샘플 포인트를 테스트할 때 이전 샘플 포인트에서 얻은 정보는 무시한다.  
하지만 베이지안 최적화는 이전의 정보들을 활용한다.  
베이지안 최적화는 미지의 목적함수의 형태를 학습하는 방법을 통해 목적함수를 전역 최적해로 만드는 파라미터를 찾는다.  
우선 사전분포에 기반하여 하나의 탐색 함수를 가정한다.  
그리고 새로운 샘플링을 사용하여 목적 함수를 테스트할 때 해당 정보를 사용하여 새로운 목적함수의 사전분포를 업데이트한다.  
마지막으로, 알고리즘은 사후분포를 통해 얻은 전역 최적해가 나타날 가능성이 가장 높은 위치에서 알고리즘을 테스트한다.  
주의할 점은, 만약 하나의 국소 최적해를 찾는다면 해당 영역에서 샘플링이 자주 일어날 것이고, 국소 최적해의 함정에 빠질 수 있다.  
이러한 단점을 보완하기 위해 베이지안 최적화는 탐색과 이용 사이의 하나의 균형점을 찾게 된다.  
탐색은 아직 샘플링하지 않은 영역에서 샘플링을 하는 것이며, 이용은 사후분포에 기반해 전역 최적해가 나타날 가능성이 높은 영역에서 샘플링하는 것을 의미한다.  

### **과적합(Over-Fitting), 과소적합(Under-Fitting)**  

모델 평가와 튜닝 과정에서 과적합과 과소적합은 자주 마주치는 문제이다.  
과적합은 모델이 훈련 데이터에 과도하게 맞춰진 현상이다.  
이는 평가 지표에 반영이 되는데, 일반적으로 훈련 데이터에 대해서는 모델의 성능이 매우 좋게 나타나지만, 테스트 데이터나 새로운 데이터상에서 성능이 저조하게 나타난다.  

과소적합은 모델이 훈련이나 예측에서 모두 좋지 못한 성능을 보이는 현상을 뜻한다.  


#### 과적합 방지  

데이터의 관점에서 본다면, 더 많은 데이터를 확보하는 것이 과적합의 위험을 낮출 수 있는 가장 효과적인 방법이다.  
샘플이 많아진다면, 모델은 더 많은 유효한 특성들을 학습할 수 있으며, 노이즈에 대한 영향이 줄어들게 된다.  
직접적으로 데이터를 늘리는 것은 쉬운 일이 아니지만, 일정한 규칙에 따라 훈련 데이터를 확보하는 작업은 충분히 가능하다.  
이미지 문제에서는 평행이동, 회전, 수축, 확장 등의 방법으로 데이터 확장을 진행할 수 있다.  
혹은 생성모델을 활용하여 대량의 신규 데이터를 추가하는 방법도 존재한다.  

데이터가 비교적 적은 상황에서 모델이 지나치게 복잡하다면 과적합이 발생할 가능성이 높아진다.  
모델의 복잡도를 적절하게 낮춰준다면 노이즈 데이터를 과도하게 학습하는 것을 방지할 수 있다.  
예를 들어, 신경망 모델에서 layer나 뉴런 수를 줄이는 방법을 이용할 수 있다.  
의사결정트리에서는 나무의 깊이를 줄이고, 가지치기를 할 수도 있다.  

모델의 파라미터에 정규화 항을 추가한다.  
대표적으로 L1, L2 정규화가 있다.  
이 방법은 기존 손실함수에 가중치를 추가해주는 것이다.  
이 방법을 사용하면 기존의 목적함수를 최적화하는 동시에 가중치가 너무 커져서 과적합 위험이 커지는 것을 어느 정도 제어할 수 있다.  

앙상블 학습은 다수의 모델을 합치는 방법인데, 이를 통해 단일 모델의 과적합 위험을 낮춰줄 수 있다.  

#### 과소적합 방지  

일반적으로 특성이 부족하거나, 특성들과의 상관성이 약할 경우 과소 적합을 일으킬 가능성이 크다.  
새로운 특성을 더 발굴한다면, 더 좋은 결과를 얻을 수 있다.  
많은 딥러닝 모델이 자동으로 이러한 피처 엔지니어링을 완성해주는 기능들을 추가해왔다.  
인수분해 머신(factorization machine), 그래디언트 부스팅 의사결정 트리(gradient boosting decision tree), Deep-crossing 등이 이러한 방법에 속한다.  

과적합 때와 반대로 모델의 복잡도를 증가시켜주는 것도 좋은 방법이다.  
간단한 모델일수록 비교적 학습 능력이 떨어진다.  
해결해야하는 문제가 복잡할수록, 모델의 복잡도를 올려주면 더 강한 적합 능력을 더해줄 수 있다.  

또한 정규화는 과적합을 방지할 때 사용되는데, 모델이 과소적합 현상을 보인다면 목적성 있게 정규화 계수를 줄여 줘야 한다.  



<br/>
