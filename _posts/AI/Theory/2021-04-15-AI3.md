---
title: "모델 평가 1"
author: Daekyo Jeong
date: 2021-04-15 15:00:00 +0900
categories: [AI, Theory]
tags: [IT, AI, Machine Learning, Deep Learning]

math: true
---

---
**Contents**

{:.no_toc}

* Will be replaced with the ToC, excluding the "Contents" header
{:toc}
---

<br/>

# **모델 평가**  

모델에 따라 평가 과정에서 서로 다른 지표를 사용하게 된다.  
이러한 평가 지표 중 대부분은 모델의 일부 성능에 대한 단편적인 부분만 반영하게 된다.  
만약 적절한 평가 지표를 사용하지 않는다면, 모델의 문제점을 발견하지 못할 수 있고 잘못된 결론을 도출할 위험이 있다.  

### **정확도**  

정확도를 이용한 평가는 가장 기본적인 평가 방법이다.  
이 방법은 다음 식과 같다.  

$$
Accuracy = \frac{n_{correct}}{n_{total}}
$$

가장 간단하고 직관적인 평가 지표이지만 명백한 단점이 존재한다.  
클래스별 샘플의 개수가 불균형한 경우, 정확도는 불균형한 정도에 많은 영향을 받게 된다.  

예를 들어, 강아지와 고양이를 판별하는 문제에서 강아지 사진은 9,900장, 고양이 사진은 100장이라고 하자.  
정확도로 모델을 평가하게 된다면, 모든 사진을 강아지라고 분류해도 이 모델의 정확도는 99%이다.  

이러한 문제점을 해결하기 위한 방법은 여러가지가 있다.  
그 중하나로 평균 정확도를 평가 지표로 활용하는 방법이 있다.  
평균 정확도는 클래스별 정확도의 평균을 나타낸다.  
위의 예시에서는 강아지 클래스의 정확도는 100%이고, 고양이 클래스의 정확도는 0%이다.  
따라서 평균 정확도는 50%가 될 것이다.  

### **Precision, Recall**  

Precision(정밀도)란 예측 결과가 양성인 것 중에서 실제 양성인 것의 비율이다.  

Recall(재현율)은 실제 양성인 것 중에 양성이라고 예측해낸 것의 비율이다.  

이러한 정밀도와 재현율을 이용한 평가에는 P-R 곡선을 그려보는 방법이 있다.  

P-R 곡선의 x축은 재현율, y축은 정밀도이다.  

이 외에도 F1 score, ROC 곡선 등을 이용하여 모델을 평가할 수 있다.  

F1 Score는 정밀도와 재현율의 조화 평균으로 수식은 다음과 같다.  

$$
F1 = \frac{2 \times precision \times recall}{precision + recall}
$$

### **RMSE(평균제곱근오차)**  

회귀 모델을 평가할 때 RMSE는 자주 사용된다.  
하지만 특정 경우 RMSE는 좋은 평가지표가 되지 못한다.  
일반적인 상황에서 RMSE는 예측값이 실제값에서 벗어난 정도를 잘 반영한다.  
하지만 벗어난 정도가 매우 큰 특이점(outlier)가 존재하는 경우, 이러한 특이점들 때문에 RMSE 지표는 매우 높아진다.  

이러한 경우는 어떻게 해결해야 할까?  

첫 번째는 이러한 특이점들이 노이즈인 경우이다.  
이 경우 전처리 과정에서 이러한 데이터들을 제거해야 한다.  

두 번째는 노이즈가 아닌 경우이다.  
이 경우 모델의 성능을 향상시켜 특이점 데이터에 대해서도 원활한 예측이 가능하도록 해야한다.  

마지막으로 다른 평가 지표를 사용하는 방법이다.  
MAPE(평균절대비오차)와 같은 평가 지표는 RMSE보다 더 견고하다고 알려져 있다.  
이러한 지표를 이용할 수도 있을 것이다.  


### **ROC(Receiver Operating Characteristic Curve) 곡선**  

이진 분류(Binary Classification) 문제는 머신러닝 분야에서 가장 자주 보이고 응용되는 문제이다.  
이진 분류의 평가 지표로 위에서 소개한 Precision, Recall, F1 score, P-R 곡선 등을 사용한다.  
하지만 이러한 평가 지표들은 모델 성능의 일부분만 반영하게 되는 한계가 명확하다.  

이에 비해 ROC 곡선은 많은 장점을 가지고 있고, 이진 분류의 성능을 평가하는 중요한 지표 중 하나이다.  

ROC 곡선의 가로축은 거짓 양성 비율(False Positive Rate, FPR)을 나타내며, 세로축은 실제 양성 비율(True Positive Rate, TPR)을 나타낸다.  

각각의 계산식은 다음과 같다.  

$$
FPR = \frac{FP}{N}  

TPR = \frac{TP}{P}  
$$

P는 실제 양성 샘플의 수, N은 실제 음성 샘플의 수를 의미하며, TP는 P개의 양성 샘플 중 분류기가 양성 샘플로 예측한 샘플의 개수, FP는 N개의 음성 샘플 중 분류기가 양성 샘플로 예측한 샘플의 개수를 나타낸다.  

그럼 이제 ROC 곡선은 어떻게 그릴까?  

ROC 곡선은 분류기의 '절단점'을 이동시키며 곡선상의 중요 지점들을 생성한다.  

일반적으로 이진 분류 문제에서 모델의 출력은 샘플이 양성일 확률이다.  
모델을 양성, 음성 값으로 출력하기 전에 임계값을 정해주고, 예측확률이 임계값보다 높다면 양성, 작다면 음성으로 분류된다.  

이 때 절단점이란 양성과 음성 결과를 구분짓는 임계값을 의미한다.  
절단점은 동적으로 조절이 가능한데, 높은 점수부터 낮은 점수로 이동시키며, 각 절단점은 모두 하나의 FPR과 TPR에 대응된다.  
대응되는 모든 위치를 연결하게 되면, 최종적으로 ROC 곡선을 얻을 수 있다.  

일반적으로 ROC 곡선은 P-R 곡선보다 데이터 세트에 대한 의존도가 낮고, 더 견고한 결과를 보여준다.  
이 말은 ROC 곡선은 다른 데이터 세트를 넣더라도, 결과의 큰 변화가 없다는 의미로, 더 객관적으로 모델 자체의 성능을 평가할 수 있도록 해준다.  

하지만 둘 중에 어떤 것을 평가 지표로 삼을지는 해결하고자 하는 문제에 따라 달라질 수 있다.  
만약 모델이 특정 데이터 세트상에서 어떤 성능을 가지는지 알고 싶다면 P-R 곡선을 선택하는 것이 더 바람직할 수 있다.  


### **AUC(Area Under Curve)**  

AUC는 ROC 곡선의 아래 면적을 의미한다.  
이 지표는 ROC 곡선에 기반하여 모델의 성능을 정량화하여 표현할 수 있다.  

AUC값을 계산하기 위해서는 ROC 곡선의 x축을 따라 적분하면 된다.  
대부분의 ROC 곡선은 y=x 선보다 높은 곳에 위치하기 때문에 일반적으로 0.5~1 사이에 있다.  
AUC가 클수록 분류기의 성능이 더 좋다는 것을 의미한다.  


<br/>
