---
title: "[프로그래머스 인공지능스쿨] Week2-5 인공지능 수학 : 추정, 검정, 엔트로피"
author: Daekyo Jeong
date: 2020-12-11 22:00:00 +0900
categories: [강의, AI]
tags: [Programmers, AI, Python]

math: true
---

# **강의**   
<br/>

### **자기정보(self-information) : i(A)**  

A : 사건  

$$
i(A) = log_{b}(\frac{1}{P(A)}) = -log_{b}P(A)
$$


$$
i(AB) = log_{b}(\frac{1}{P(A)P(B)}) = log_{b}(\frac{1}{P(A)}) + log_{b}(\frac{1}{P(B)}) = i(A) + i(B)
$$

### **엔트로피(entropy)**  

자기정보의 평균  

$$
H(X) = \sum_{j}P(A_{j})i(A_{j}) = -\sum_{j}P(A_{j})log_{2}P(A_{j})
$$

$$
0 \le H(X) \le log_{2}K (K: 사건의 수)
$$


### **교차 엔트로피**  

집합 S상에서 확률 분포 P에 대한 확률분포 Q의 교차 엔트로피  

확률 분포 P에서 $$i(A_{j})$$의 평균  

$$
H(P,Q) = \sum_{j}P(A_{j})i(A_{j}) = -\sum_{j}P(A_{j})log_{2}Q(A_{j})  
$$

이 값은 P와 Q가 얼마나 비슷한지를 표현  

같으면 $$H(P,Q) = H(P)$$  

다르면 $$H(P,Q) > H(P)$$  

##### **분류 문제에서의 손실함수에 사용됨**  


원하는 답 $$P = [p_{1}, p_{2}, ..., p_{n}], p_{1} + p_{2} + ... + p_{n} = 1$$  

제시된 답 $$Q = [q_{1}, q_{2}, ..., q_{n}], q_{1} + q_{2} + ... + q_{n} = 1$$  

P와 Q가 얼마나 다른지에 대한 척도 필요  

- 제곱합  

$$
\sum(p_{i} - p_{j})^2
$$

확률이 다를수록 큰 값을 가짐, 학습 속도가 느림  

- 교차 엔트로피 H(P, Q)

확률이 다를수록 큰값을 가지며, 학습 속도가 빠름  
따라서 분류문제에서 주로 교차 엔트로피를 사용함  




<br/>
