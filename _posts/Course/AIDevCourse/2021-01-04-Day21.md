---
title: "[프로그래머스 인공지능스쿨] Week5-1 Machine Learning 기초 : 확률이론"
author: Daekyo Jeong
date: 2021-01-04 00:00:00 +0900
categories: [Course, AI Dev Course]
tags: [Programmers, AI, Python]

math: true
---

# **강의**   
<br/>

## **Machine Learning 기초**

### **$$\rhd$$ Machine Learning 이란?**  

경험을 통해 자동으로 개선하는 컴퓨터 알고리즘으로,  
학습 데이터를 이용해 목표값을 예측한다.  

### **$$\rhd$$  Machine Learning 기초 개념**  

학습 단계: 함수 y(x)를 학습 데이터에 기반해 결정하는 단계  
시험 셋: 모델을 평가하기 위해서 사용하는 새로운 데이터  
일반화: 모델에서 학습에 사용된 데이터가 아닌 새로운 데이터에 대해 올바른 예측을 수행하는 역량  
지도학습: target이 주어진 경우
- 분류(classification)
- 회귀(regression)  

비지도학습: target이 없는 경우
- 군집(clustering)

### **$$\rhd$$  과소적합(under-fitting)과 과대적합(over-fitting)**  

- 과소적합(under-fitting)  

부실한 학습으로 인하여 데이터를 제대로 예측하지 못하는 경우이다.

- 과대적합(over-fitting)  

무리한 학습으로 인하여 데이터의 노이즈까지 과도하게 학습한 경우이다.  
주어진 데이터에 대해서는 높은 정확도를 보이지만, 새로운 데이터에 대해서는 좋지 않은 결과를 보일 가능성이 높다.  

## **확률 이론**  

### **$$\rhd$$ 확률 변수(Random Variable)**  

확률 변수 X는 표본 집합 S의 원소 e를 실수 값 $ X(e) = x$ 에 대응시키는 함수이다.  

- 대문자 X, Y ... : 확률 변수
- 소문자 x, y ... : 확률 변수가 가질 수 있는 값  
- 확률 P는 집합 S의 부분집합을 실수값에 대응시키는 함수이다.

### **$$\rhd$$ 확률 변수의 성질**  

- 덧셈법칙  

$$
p(X) = \sum_{Y}p(X,Y)
$$

- 곱셈법칙  

$$
p(X,Y)=p(X|Y)p(Y) = p(Y|X)p(X)
$$

- 베이즈 확률  

$$
p(Y|X) = \frac{p(X|Y)p(Y)}{\sum_{Y}p(X|Y)p(Y)}  
$$

$$
posterior = \frac{likelihood \times prior}{normalization}
$$

posterior : 사후 확률  
likelihood: 가능도(우도)  
prior: 사전확률  
normalization: Y와 상관없는 상수. X의 경계확률(marginal) p(X)

### **$$\rhd$$ 확률 변수의 함수**  

확률 변수 X의 함수 Y = g(X)와 역함수 w(Y) = X 가 주어졌을 때 다음이 성립한다.  

$$
p_{y}(Y) = p_{x}(x)|\frac{dx}{dy}|
$$

### **$$\rhd$$ 정규분포(Gaussian Distribution)**  

단일 변수 x를 위한 가우시안 분포  

$$
N(x|\mu, \sigma^{2}) = \frac{1}{(2\pi\sigma^{2})^{1/2}}exp\{-\frac{1}{2\sigma^{2}}(x-\mu)^{2}\}
$$

$$
\int^{\infty}_{-\infty}N(x|\mu, \sigma^{2})dx = 1
$$


<br/>
