---
title: "[프로그래머스 인공지능스쿨] Week8-5 Deep Learning: 순환신경망 - RNN"
author: Daekyo Jeong
date: 2021-01-29 00:00:00 +0900
categories: [강의, AI]
tags: [Programmers, AI, Python]

math: true
---

# **강의**   
<br/>

## **시계열 데이터(Time series data)**  

특징들이 순서를 가지기 때문에, 단순히 공간적인 측면이외에도, 순서에 따른 특징이 중요함  
시계열 데이터는 보통 동적이며 길이가 가변적임

### **$$RNN(Recurrent neural networks)**  

RNN은 시계열 정보를 처리하는데 효과적인 학습 모델이다.  
매우 긴 시계열 데이터를 처리할 때는, 장기 의존성(long term dependency)를 잘 다룰 수 있는 LSTM을 주로 사용한다.  

최근에는 순환 신경망도 생성 모델로 사용하기도 한다.  

### **$$\rhd$$ RNN 필수 요소**  

1. **시간성** : 특징을 순서대로 한 번에 하나씩 입력받는다.  
2. **가변 길이** : 길이가 T인 샘플을 처리하려면, 은닉층이 T번 나타나야 한다.  
3. **문맥 의존성** : 이전 특징 내용을 기억하고 있다가 적절한 순간에 활용해야 한다.  

### **$$\rhd$$ RNN 구조**  

기존 깊은 신경망과 유사하게 입력층, 은닉층, 출력층을 가진다.  
다른 점은 은닉층이 순환 연결(Recurrent edge)를 가진다.  
순환 연결은 t-1 시간에 발생한 정보를 t 시간으로 전달하는 역할을 한다.  
이러한 순환 연결을 통해 시간성, 가변 길이, 문맥 의존성을 모두 처리할 수 있다.  


U: 입력층과 은닉층 사이의 가중치  
V: 은닉층과 출력층 사이의 가중치  
W: 은닉층 사이의 가중치  

RNN은 매개변수를 공유한다.  
매 순간 다른 값을 사용하지 않고 같은 값을 공유한다.  
이를 통해 매개변수 수가 현저하게 줄어들었고, 특징이 나타나는 시간이 뒤바뀌어도 같거나 유사한 출력을 만들 수 있다.  

### **$$\rhd$$ 양방향 RNN(Bidirectional RNN)**  

기존 RNN의 경우 t 시간의 데이터는 t시간 이전의 데이터로부터 전달된 정보를 보고 처리되었다.  
양방향 RNN의 경우 앞쪽, 뒤쪽 정보를 모두 보고 처리한다.  

### **$$LSTM(Long short term memory)**  

기존의 RNN의 경우 장기 문맥 의존성(Long-term dependency)이 보존되기 어렵다.  
거리가 멀어질수록 경사 소멸이나 경사 폭발이 발생한다.  
이 문제는 긴 입력 샘플이나 가중치 공유라는 특성때문에 CNN이나 MLP보다 더 심각하다.  

이러한 문제점을 개선하고자 나온 모델이 LSTM이다.  
LSTM은 입력 개폐구와 출력 개폐구를 통해 신호를 부분적으로 받는다.  
개폐구를 열면 신호가 흐르고, 닫으면 차단되는 방식이다.  
해당 개폐의 정도는 학습을 통해 찾아간다.  

### **$$\rhd$$ LSTM 핵심 요소**  

- **메모리 블록(셀)** : 은닉 상태 장기 기억  
- **망각 개폐구(forget gate)** : 기억 유지 혹은 제거(1:유지, 0:제거)  
- **입력 개폐구(input gate)** : 입력 연산  
- **출력 개폐구(output gate)** : 출력 연산  
