---
title: "[프로그래머스 인공지능스쿨] Week8-3 Deep Learning: 최적화 - 손실함수"
author: Daekyo Jeong
date: 2021-01-27 00:00:00 +0900
categories: [Course, AI Dev Course]
tags: [Programmers, AI, Python]

math: true
---

# **강의**   
<br/>

학습을 마친 후, 실생활에서 발생하는 새로운 샘플들도 잘 예측할 수 있어야 한다.  
즉, 일반화하는 능력이 좋아야한다.  
이러기 위한 다양한 최적화 방법을 알아보자.  

## **손실 함수**  

### **$$\rhd$$ 평균 제곱 오차(MSE: Mean Square Error)**  

$$
e = \frac{1}{2}\parallel y - o\parallel_{2}^{2}
$$

y는 실제 결과, o는 출력값이다.  
오차(e)가 클수록 가중치와 편향을 더 크게 교정해준다.  
하지만 wx+b가 커지면, 경사도가 작아지기 때문에 오차가 큰 상황에서도 더 적게 교정을하는 문제가 있다.  



### **$$\rhd$$ 교차 엔트로피(Cross Entropy)**  

$$
H(P,Q) = -\sum_{i=1,...,k}P(e_{i})log_{2}Q(e_{i})
$$

$$
e = -(ylog_{2}o + (1-y)log_{2}(1-o))
$$

교차 엔트로피는 위의 MSE에서 문제점을 해결하여 MSE의 느린 학습문제를 해결함  


### **$$\rhd$$ 음의 로그우도(Negative log-likelihood)**  

$$
e = -log_{2}o_{y}
$$

모든 출력 노드값에 적용하는 MSE나 교차 엔트로피와 달리 정답노드 하나에만 적용한다.    
소프트맥스 활성 함수는 정답값은 1에 가깝게 만들고, 이외의 값은 0에 가깝게 만들기 때문에, 로그우도와 결합하여 사용하는 경우가 많다.  

우리가 다루는 대부분의 분류문제는 one-hot encoding으로 이루어져 있기 때문에, 사실상 교차 엔트로피와 로그우드의 수식은 같아진다.  
